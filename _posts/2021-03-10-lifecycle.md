---
layout: page
title: The Life-Cycle of AI Art
summary: How AI art evolves from technology to creativity
author:  AaronHertzmann
---



# The Life-Cycle of AI Art

A fascinating new process for the emergence of AI art techniques is beginning to appear. I've alluded to this process in [previous papers](https://cacm.acm.org/magazines/2020/5/244330-computers-do-not-make-art-people-do/fulltext), but now that we're seeing the process repeat, it seems possible to make more concrete statements about it.

Because of the Twitter and GitHub repos, we have a sort of worldwide collaboration of artists and technologists happening, at an extraordinary pace. While there is a long history of artists and technologists working together, and of individuals who innovate in both art and science simultaneously, this distributed international collaboration is—dare I say—_completely unprecedented_.

This collaboration is an extension of the revolution that has happened in machine learning and computer vision research: the use of arXiv and GitHub and a widespread open sharing approach to research has enabled artists to easily poke about with tools meant for researchers and developers. There are some major downsides to the way this research has been conducted, but we can all agree that it has enabled technical development at a truly staggering pace, which have led to rapid development of new ideas in digital art.

Some of the most significant AI algorithms where this has happened are: DeepDream, StyleGAN, pix2pix, CycleGAN, BigGAN, SPADE, and now DALL-E and CLIP.

The phases in this cycle seem to be:

1. **Computer science researchers release image generation code** on GitHub, usually accompanying a technical paper posted to arXiv or published at a conference.  Sometimes these papers say little or nothing explicit about making art; they are focused on technical and algorithmic problems, even though the paper figures are often inspiring or delightful.

2. **The links get widely shared on Twitter,** sometimes together with news articles and press releases about the technology.

3. **Artists and other tinkerers download the code and experiment with the technology,** kicking its tires, experimenting with different ways to use the technology to make images. These are tech-savvy artists, skilled in coding with the ML tools, and generally willing to play in the mud. These artists sometimes post their first experiments _within days_ of the ML code first being released to the public.  

For example, [here's a review of Mario Klingemann's feed of BigGAN experiments](https://hyperallergic.com/481969/an-ai-artists-twitter-feed-is-an-art-gallery/)

A lot of these experiments are whimsical, just playing around, but play is an important part of exploration.


4. **Artists begin to release new work that uses these tools, showing it in galleries and exhibitions.** DeepDream, StyleGAN, and BigGAN have all been used in fine art exhibitions. DALL-E and CLIP are so new that they haven't yet, but it's only a matter of time now.

5. **Enough artists use these tools in straightforward ways that the style of the technology becomes recognizable and predictable.** DeepDream was briefly amazing and then became  boring pretty fast. GANs are a much richer space, but there is a lot of GAN-based art out there that all looks the same, and, I for one have lost interest in much of it. [GAN fatigue](https://www.mitpressjournals.org/doi/abs/10.1162/leon_a_01930) has set in.  However, most of the world hasn't seen GAN art, so there is still a considerable potential audience for it.

6. **The technology either matures or fades away.** I'm still enjoying Helena Sarin and Sofia Crespo's latest experiments with GANs, which are far more interesting than vanilla GAN renderings. 
On the other hand, DeepDream has completely faded from view.  


7. **Then, eventually, some exciting new algorithm is released** and we go back to step one.

A few things worth noting about this process:

**The process is cumulative.** Even though the cycle seems to be largely over for BigGAN and StyleGAN, the collective knowledge of these tools remains. I think the [indeterminate visual aesthetic artists found in them]() will persist in new algorithms.

Moreover, **newer experiments mix-and-match ideas from these older ideas**.  [AI artists are generative artists, and writing code to make art—whether with AI algorithms or not—is a tradition that goes back 60 years.](https://www.artnews.com/art-in-america/features/generative-art-tools-flash-processing-neural-networks-1202674657/). BigGAN isn't gone, it's still a tool in the AI artists' toolbox. 
Here are some combinations of tools that were just shared in the past few days:

<center>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">neuron #2478 &quot;surprised&quot;<br>exported to CPPN then GLSL and slightly animated <a href="https://t.co/j7kyURlxkc">https://t.co/j7kyURlxkc</a><br>NB: 13k parameters, may crash some GPU<br>[wtf twitter doesn&#39;t show shadertoy previews !?]<br><br>original technique by <a href="https://twitter.com/wxswxs?ref_src=twsrc%5Etfw">@wxswxs</a><a href="https://t.co/zYbLtHpXvk">https://t.co/zYbLtHpXvk</a> <a href="https://t.co/HWTRt9BAJI">pic.twitter.com/HWTRt9BAJI</a></p>&mdash; vadim epstein (@eps696) <a href="https://twitter.com/eps696/status/1369460655846264835?ref_src=twsrc%5Etfw">March 10, 2021</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</center>

<center>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Okay, now I&#39;m just being silly. Seriously. <a href="https://t.co/NMrhBzL8kV">pic.twitter.com/NMrhBzL8kV</a></p>&mdash; Doron Adler (@Norod78) <a href="https://twitter.com/Norod78/status/1369546359695630337?ref_src=twsrc%5Etfw">March 10, 2021</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</center>

Watching great new forms of art develop is a wonder to behold, and it's happening now, live, on social media, arXiv, GitHub, and beyond.